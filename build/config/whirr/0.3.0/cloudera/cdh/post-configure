#!/usr/bin/env bash
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Install CDH.
#

set -x
set -e

################################################################################
# Initialize variables
################################################################################

ROLES=$1
shift

NN_HOST=
JT_HOST=
CLOUD_PROVIDER=
while getopts "n:j:c:" OPTION; do
  case $OPTION in
  n)
    NN_HOST="$OPTARG"
    ;;
  j)
    JT_HOST="$OPTARG"
    ;;
  c)
    CLOUD_PROVIDER="$OPTARG"
    ;;
  esac
done

case $CLOUD_PROVIDER in
  ec2)
    # Use public hostname for EC2
    SELF_HOST=`wget -q -O - http://169.254.169.254/latest/meta-data/public-hostname`
    ;;
  *)
    SELF_HOST=`/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}'`
    ;;
esac

REPO=${REPO:-cdh3}
HADOOP=hadoop-${HADOOP_VERSION:-0.20}
HADOOP_CONF_DIR=/etc/$HADOOP/conf.dist
for role in $(echo "$ROLES" | tr "," "\n"); do
  case $role in
  nn)
    NN_HOST=$SELF_HOST
    ;;
  jt)
    JT_HOST=$SELF_HOST
    ;;
  esac
done

# Install a list of packages on debian or redhat as appropriate
function install_packages() {
  if which dpkg &> /dev/null; then
    apt-get update
    apt-get -y install $@
  elif which rpm &> /dev/null; then
    yum install -y $@
  else
    echo "No package manager found."
  fi
}

function prep_disk() {
  mount=$1
  device=$2
  automount=${3:-false}

  echo "warning: ERASING CONTENTS OF $device"
  mkfs.xfs -f $device
  if [ ! -e $mount ]; then
    mkdir $mount
  fi
  mount -o defaults,noatime $device $mount
  if $automount ; then
    echo "$device $mount xfs defaults,noatime 0 0" >> /etc/fstab
  fi
}

function make_hadoop_dirs {
  for mount in "$@"; do
    if [ ! -e $mount/hadoop ]; then
      mkdir -p $mount/hadoop
      chgrp hadoop $mount/hadoop
      chmod g+w $mount/hadoop
    fi
  done
}

# Configure Hadoop by setting up disks and site file
function configure_hadoop() {
  case $CLOUD_PROVIDER in
  ec2)
    MOUNT=/mnt
    ;;
  *)
    MOUNT=/data
    ;;
  esac
  FIRST_MOUNT=$MOUNT
  DFS_NAME_DIR=$MOUNT/hadoop/hdfs/name
  FS_CHECKPOINT_DIR=$MOUNT/hadoop/hdfs/secondary
  DFS_DATA_DIR=$MOUNT/hadoop/hdfs/data
  MAPRED_LOCAL_DIR=$MOUNT/hadoop/mapred/local
  MAX_MAP_TASKS=2
  MAX_REDUCE_TASKS=1
  CHILD_MAP_OPTS=-Xmx550m
  CHILD_REDUCE_OPTS=-Xmx550m
  CHILD_MAP_ULIMIT=1126400
  CHILD_REDUCE_ULIMIT=1126400
  TMP_DIR=$MOUNT/tmp/hadoop-\${user.name}

  mkdir -p $MOUNT/hadoop
  chgrp hadoop $MOUNT/hadoop
  chmod g+w $MOUNT/hadoop
  mkdir $MOUNT/tmp
  chmod a+rwxt $MOUNT/tmp

  ##############################################################################
  # Modify this section to customize your Hadoop cluster.
  ##############################################################################
  cat > $HADOOP_CONF_DIR/hadoop-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
  <name>dfs.block.size</name>
  <value>134217728</value>
  <final>true</final>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>$DFS_DATA_DIR</value>
  <final>true</final>
</property>
<property>
  <name>dfs.datanode.du.reserved</name>
  <value>1073741824</value>
  <final>true</final>
</property>
<property>
  <name>dfs.datanode.handler.count</name>
  <value>3</value>
  <final>true</final>
</property>
<!--property>
  <name>dfs.hosts</name>
  <value>$HADOOP_CONF_DIR/dfs.hosts</value>
  <final>true</final>
</property-->
<!--property>
  <name>dfs.hosts.exclude</name>
  <value>$HADOOP_CONF_DIR/dfs.hosts.exclude</value>
  <final>true</final>
</property-->
<property>
  <name>dfs.name.dir</name>
  <value>$DFS_NAME_DIR</value>
  <final>true</final>
</property>
<property>
  <name>dfs.namenode.handler.count</name>
  <value>5</value>
  <final>true</final>
</property>
<property>
  <name>dfs.permissions</name>
  <value>true</value>
  <final>true</final>
</property>
<property>
  <name>dfs.replication</name>
  <value>$DFS_REPLICATION</value>
</property>
<property>
  <name>fs.checkpoint.dir</name>
  <value>$FS_CHECKPOINT_DIR</value>
  <final>true</final>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://$NN_HOST:8020/</value>
</property>
<property>
  <name>fs.trash.interval</name>
  <value>1440</value>
  <final>true</final>
</property>
<property>
  <name>hadoop.tmp.dir</name>
  <value>$MOUNT/tmp/hadoop-\${user.name}</value>
  <final>true</final>
</property>
<property>
  <name>io.file.buffer.size</name>
  <value>65536</value>
</property>
<property>
  <name>mapred.map.child.java.opts</name>
  <value>$CHILD_MAP_OPTS</value>
</property>
<property>
  <name>mapred.reduce.child.java.opts</name>
  <value>$CHILD_REDUCE_OPTS</value>
</property>
<property>
  <name>mapred.map.child.ulimit</name>
  <value>$CHILD_MAP_ULIMIT</value>
  <final>true</final>
</property>
<property>
  <name>mapred.reduce.child.ulimit</name>
  <value>$CHILD_REDUCE_ULIMIT</value>
  <final>true</final>
</property>
<property>
  <name>mapred.job.tracker</name>
  <value>$JT_HOST:8021</value>
</property>
<property>
  <name>mapred.job.tracker.handler.count</name>
  <value>5</value>
  <final>true</final>
</property>
<property>
  <name>mapred.local.dir</name>
  <value>$MAPRED_LOCAL_DIR</value>
  <final>true</final>
</property>
<property>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
</property>
<property>
  <name>mapred.reduce.parallel.copies</name>
  <value>10</value>
</property>
<property>
  <name>mapred.reduce.tasks</name>
  <value>10</value>
</property>
<property>
  <name>mapred.reduce.tasks.speculative.execution</name>
  <value>false</value>
</property>
<property>
  <name>mapred.submit.replication</name>
  <value>10</value>
</property>
<property>
  <name>mapred.system.dir</name>
  <value>/hadoop/system/mapred</value>
</property>
<property>
  <name>mapreduce.jobtracker.staging.root.dir</name>
  <value>/user</value>
</property>
<property>
  <name>mapred.tasktracker.map.tasks.maximum</name>
  <value>$MAX_MAP_TASKS</value>
  <final>true</final>
</property>
<property>
  <name>mapred.tasktracker.reduce.tasks.maximum</name>
  <value>$MAX_REDUCE_TASKS</value>
  <final>true</final>
</property>
<property>
  <name>tasktracker.http.threads</name>
  <value>46</value>
  <final>true</final>
</property>
<property>
  <name>mapred.compress.map.output</name>
  <value>true</value>
</property>
<property>
  <name>mapred.output.compression.type</name>
  <value>BLOCK</value>
</property>
<property>
  <name>hadoop.rpc.socket.factory.class.default</name>
  <value>org.apache.hadoop.net.StandardSocketFactory</value>
  <final>true</final>
</property>
<property>
  <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
  <value></value>
  <final>true</final>
</property>
<property>
  <name>hadoop.rpc.socket.factory.class.JobSubmissionProtocol</name>
  <value></value>
  <final>true</final>
</property>
<property>
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec</value>
</property>
<!-- Hue configuration -->
<property>
  <name>dfs.namenode.plugins</name>
  <value>org.apache.hadoop.thriftfs.NamenodePlugin</value>
  <description>Comma-separated list of namenode plug-ins to be activated.
  </description>
</property>
<property>
  <name>dfs.datanode.plugins</name>
  <value>org.apache.hadoop.thriftfs.DatanodePlugin</value>
  <description>Comma-separated list of datanode plug-ins to be activated.
  </description>
</property>
<property>
  <name>dfs.thrift.address</name>
  <value>0.0.0.0:9090</value>
</property>
<property>
  <name>jobtracker.thrift.address</name>
  <value>0.0.0.0:9290</value>
</property>
<property>
  <name>mapred.jobtracker.plugins</name>
  <value>org.apache.hadoop.thriftfs.ThriftJobTrackerPlugin</value>
  <description>Comma-separated list of jobtracker plug-ins to be activated.</description>
</property>
</configuration>
EOF

# Expose /metrics URL endpoint
  cat > $HADOOP_CONF_DIR/hadoop-metrics.properties <<EOF
# Exposes /metrics URL endpoint for metrics information.
dfs.class=org.apache.hadoop.metrics.spi.NoEmitMetricsContext
mapred.class=org.apache.hadoop.metrics.spi.NoEmitMetricsContext
jvm.class=org.apache.hadoop.metrics.spi.NoEmitMetricsContext
rpc.class=org.apache.hadoop.metrics.spi.NoEmitMetricsContext
EOF

  # Set SSH options within the cluster
  sed -i -e 's|# export HADOOP_SSH_OPTS=.*|export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no"|' \
    $HADOOP_CONF_DIR/hadoop-env.sh
    
  # Disable IPv6
  sed -i -e 's|# export HADOOP_OPTS=.*|export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"|' \
    $HADOOP_CONF_DIR/hadoop-env.sh

  # Hadoop logs should be on the /mnt partition
  rm -rf /var/log/hadoop-0.20
  mkdir -p $MOUNT/hadoop/logs
  chmod g+w $MOUNT/hadoop/logs
  chgrp -R hadoop $MOUNT/hadoop/logs
  ln -s $MOUNT/hadoop/logs /var/log/hadoop-0.20
  chgrp -R hadoop /var/log/hadoop /var/log/hadoop-0.20

  echo "export HADOOP_HOME=/usr/lib/hadoop" >> /etc/profile
  echo "export HADOOP_CONF=/etc/hadoop" >> /etc/profile

}

function install_hue() {
  if which dpkg &> /dev/null; then
    apt-get -y install hue-common
    apt-get -y install hue-useradmin hue-jobsub hue-beeswax
  elif which rpm &> /dev/null; then
    yum install -y hue-common
    yum install -y hue-useradmin hue-jobsub hue-beeswax
  fi
  
  # Configure hue
  sed -i -e "s|http_port=8088|http_port=80|" /etc/hue/hue.ini
  
  # Hue logs should be on the /mnt partition
  mv /var/log/hue /var/log/hue.tmp
  mkdir -p $MOUNT/hue/logs
  chown hue:hue $MOUNT/hue/logs
  ln -s $MOUNT/hue/logs /var/log/hue
  chown -R hue:hue /var/log/hue
  for files in /var/log/hue.tmp/*; do
    if [ -a $files ]; then
     mv $files /var/log/hue
    fi
  done
  rm -rf /var/log/hue.tmp
}

function install_hue_plugins() {
  if which dpkg &> /dev/null; then
    apt-get -y install hue-plugins
  elif which rpm &> /dev/null; then
    yum install -y hue-plugins
  fi
}

function start_hue() {
  /etc/init.d/hue start
}

function start_namenode() {
  if which dpkg &> /dev/null; then
    apt-get -y install $HADOOP-namenode
    AS_HDFS="su -s /bin/bash - hdfs -c"
    # Format HDFS
    [ ! -e $FIRST_MOUNT/hadoop/hdfs ] && $AS_HDFS "$HADOOP namenode -format"
  elif which rpm &> /dev/null; then
    yum install -y $HADOOP-namenode
    AS_HDFS="/sbin/runuser -s /bin/bash - hdfs -c"
    # Format HDFS
    [ ! -e $FIRST_MOUNT/hadoop/hdfs ] && $AS_HDFS "$HADOOP namenode -format"
  fi

  service $HADOOP-namenode start

  $AS_HDFS "$HADOOP dfsadmin -safemode wait"
  $AS_HDFS "/usr/bin/$HADOOP fs -mkdir /user"
  # The following is questionable, as it allows a user to delete another user
  # It's needed to allow users to create their own user directories
  $AS_HDFS "/usr/bin/$HADOOP fs -chmod +w /user"
  $AS_HDFS "/usr/bin/$HADOOP fs -mkdir /hadoop"
  $AS_HDFS "/usr/bin/$HADOOP fs -chmod +w /hadoop"
  $AS_HDFS "/usr/bin/$HADOOP fs -mkdir /mnt"
  $AS_HDFS "/usr/bin/$HADOOP fs -chmod +w /mnt"

  # Create temporary directory for Pig and Hive in HDFS
  $AS_HDFS "/usr/bin/$HADOOP fs -mkdir /tmp"
  $AS_HDFS "/usr/bin/$HADOOP fs -chmod +w /tmp"
  $AS_HDFS "/usr/bin/$HADOOP fs -mkdir /user/hive/warehouse"
  $AS_HDFS "/usr/bin/$HADOOP fs -chmod +w /user/hive/warehouse"
}

function start_daemon() {
  daemon=$1
  if which dpkg &> /dev/null; then
    apt-get -y install $HADOOP-$daemon
  elif which rpm &> /dev/null; then
    yum install -y $HADOOP-$daemon
  fi
  service $HADOOP-$daemon start
}

# Install thrift (dependency for HBase)
function install_thrift() {
  #yum -y install hadoop-hbase-master.noarch hadoop-hbase-regionserver.noarch
  #yum -y install hadoop-hbase.noarch hadoop-hbase-thrift.noarch hadoop-zookeeper-server.noarch
  wget -q -nv -O /tmp/thrift.tar.gz http://apache.ziply.com//incubator/thrift/0.5.0-incubating/thrift-0.5.0.tar.gz
  cd /tmp/ && tar zxvf thrift.tar.gz
  cd thrift*
  yum -y install boost-devel.x86_64 python-devel.x86_64 ruby-devel.x86_64
  ./configure --enable-gen-php=no --with-php=no --with-php_extension=no
  make install
  #cd /tmp && rm -rf thrift*
  echo "/usr/local/lib" > /etc/ld.so.conf.d/thrift.conf
  echo export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/ >> /etc/bashrc
  ldconfig
  #service hadoop-zookeeper start
  #service hadoop-hbase-thrift start
}

function install_revo() {
   pushd /tmp
   # install community edition
   FILE=Revo-Co-4.3-RHEL5.tar.gz

   # download all the packages first, before we start installing
   wget -q -nv http://revo-whirr-downloads.s3.amazonaws.com/$FILE
   wget -q -nv http://revo-whirr-downloads.s3.amazonaws.com/rhdfs_0.1.tar.gz
   wget -q -nv http://revo-whirr-downloads.s3.amazonaws.com/rhstream_0.1.tar.gz
   wget -q -nv http://revo-whirr-downloads.s3.amazonaws.com/rhbase_0.1.tar.gz

   # unpack and install the main Revo R package
   tar xzf $FILE
   pushd RevolutionR_4.3; ./install.py -n -d; popd

   # make sure we have JAVA_HOME set correctly
   source /etc/profile
   source /etc/bashrc

   sh -c "Revo64 CMD javareconf && Revoscript -e 'install.packages(\"rJava\")'"

   # install rhadoop packages
   Revo64 CMD INSTALL rhdfs_0.1.tar.gz
   Revo64 CMD INSTALL rhstream_0.1.tar.gz
   Revo64 CMD INSTALL rhbase_0.1.tar.gz
   popd

   yum -y install yum-utils
}

configure_hadoop
#install_hue_plugins

for role in $(echo "$ROLES" | tr "," "\n"); do
  case $role in
  nn)
    #install_hue
    start_namenode
    #start_hue
    ;;
  snn)
    start_daemon secondarynamenode
    ;;
  jt)
    start_daemon jobtracker
    ;;
  dn)
    start_daemon datanode
    ;;
  tt)
    start_daemon tasktracker
    ;;
  esac
done

install_thrift
install_revo
